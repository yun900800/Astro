---
title: OpenClaw核心原理学习(二)——语义化视觉理解以及实现
description: 语义化视觉理解是一个生疏的概念，要这样这样的概念也是很难的，这在大模型没有出现之前是很难的，但是在大模型出现以后就可以实现了，但是还是很难的
publishDate: 2026-02-23
tags:
  - OpenClaw
  - AI
  - 语义化视觉理解
  - 意图，操作序列
ogImage: /social-card.avif
---
语义化视觉理解我可以打个比方，其实最简单的比喻就是我们人类，当我看到一个苹果，我就想到了吃，也可以想到牛顿，简单吧，归纳起来就是看到一幅画面就产生的一个行为，这里的画面可以认为就是一个苹果，产生的行为要么是吃，要么就是联想。

其实这里本是上要实现**图形界面智能体**

GUI Agent（图形界面智能体）最核心的两个工程挑战：**视觉到坐标的映射**以及**任务到动作的对齐**。

## **1. 语义化视觉理解：从“像素”到“结构化描述”**

画布图片输入大模型后输出什么”，在 OpenClaw 这种项目中，通常不是简单的文字描述，而是带坐标的结构化元数据。

### 实现路径：视觉重构

目前主流的实现方式有两种逻辑：

* 逻辑 A：纯视觉推理（Set-of-Mark, SoM）
  这是目前最火的方案。在把图片发给大模型（如 GPT-4o 或 Claude 3.5 Sonnet）之前，先用一个轻量级的检测模型（如 YOLO 或 Segment Anything）把屏幕上所有的按钮、输入框、图标都圈出来，并在每个物体上标注一个数字编号。

  * 输出内容： 大模型看到的不是一张裸图，而是一张“满是编号”的图。大模型输出：“我观察到编号 5 是‘搜索框’，编号 12 是‘确定按钮’”。
* 逻辑 B：像素坐标预测（Pixel-to-Coordinates）
  像 Google 的 Pix2Struct 或 DeepMind 的一些模型，直接训练模型输出特定格式的文本，例如：`{"element": "button", "label": "login", "bbox": [ymin, xmin, ymax, xmax]}`。

  * 输出内容： 模型直接返回一个 JSON 列表，描述图中所有关键元素的类型、标签和它们在屏幕上的绝对/相对坐标。

## **2. 语义拆解原子命令：从“意图”到“操作序列”**

理解了“图里有什么”之后，如何把“帮我买张票”拆解成点击和输入？这本质上是一个状态机（State Machine）的规划问题。

### 实现逻辑：ReAct 循环 (Reasoning and Acting)

OpenClaw 的核心算法通常遵循以下闭环：

1. 观察 (Observe)： 获取当前截图，识别出可交互元素（如上文所述）。
2. 思考 (Thought)： 大模型根据“最终目标”和“当前屏幕状态”进行推理。

   * *示例推理：* “我的目标是买票，现在屏幕在首页，我应该先点击搜索框输入目的地。”
3. 行动 (Action)： 模型输出一个预定义的原子指令。

   * 原子指令集： 通常是一组标准函数，如 `click(x, y)`、`type_text("str")`、`scroll(direction)`、`wait()`。
4. 执行与反馈： 脚本执行点击，屏幕跳转。
5. 重复： 再次截屏，进入下一个循环，直到检测到“任务完成”的视觉特征。

### 难点攻克：如何保证拆解准确？

* Few-shot Prompting： 给大模型几个“标准案例”，告诉它：“当你在登录页想登录时，先执行 type(username)，再执行 type(password)，最后 click(submit)”。
* 自我修正逻辑： 如果模型输出一个指令后，截屏发现页面没变（比如点歪了或按钮没反应），逻辑层会捕获这个异常，让模型重新规划。



## 总结：OpenClaw 的本质公式

我们可以把这个过程总结为一个简单的公式：

![OpenClaw 的本质公式](/assets/images/wechatimg43.jpg "OpenClaw 的本质公式")
